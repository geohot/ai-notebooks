{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c93bf851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the torch version works better than tensorflow\n",
    "# for good progressbar\n",
    "# jupyter nbextension install --py widgetsnbextension --user\n",
    "# jupyter nbextension enable widgetsnbextension --user --py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12fb79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8638e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataModule(pl.LightningDataModule):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    self.ds_X, self.ds_Y = self.get_dataset(*args, **kwargs)\n",
    "    shuffler = np.random.permutation(self.ds_X.shape[0])\n",
    "    self.ds_X = self.ds_X[shuffler]\n",
    "    self.ds_Y = self.ds_Y[shuffler]\n",
    "    self.split = int(self.ds_X.shape[0]*0.8)\n",
    "    \n",
    "  def train_dataloader(self):\n",
    "    ds_X_train, ds_Y_train = self.ds_X[0:self.split], self.ds_Y[0:self.split]\n",
    "    return torch.utils.data.DataLoader(list(zip(ds_X_train, ds_Y_train)), batch_size=32)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    ds_X_test, ds_Y_test = self.ds_X[self.split:], self.ds_Y[self.split:]\n",
    "    return torch.utils.data.DataLoader(list(zip(ds_X_test, ds_Y_test)), batch_size=32)\n",
    "\n",
    "class ReverseDataModule(BaseDataModule):\n",
    "  def get_dataset(self, cnt=10000, seq_len=6):\n",
    "    ds = np.random.randint(0, 10, size=(cnt, seq_len))\n",
    "    return ds, ds[:, ::-1].ravel().reshape(cnt, seq_len)\n",
    "  \n",
    "# dataset idea from https://github.com/karpathy/minGPT/blob/master/play_math.ipynb\n",
    "class AdditionDataModule(BaseDataModule):\n",
    "  def get_dataset(self):\n",
    "    ret = []\n",
    "    for i in range(100):\n",
    "      for j in range(100):\n",
    "        s = i+j\n",
    "        ret.append([i//10, i%10, j//10, j%10, s//100, (s//10)%10, s%10])\n",
    "    ds = np.array(ret)\n",
    "    return ds[:, 0:6], np.copy(ds[:, 1:])    \n",
    "\n",
    "# this is the hardest to learn and requires 4 layers\n",
    "class ParityDataModule(BaseDataModule):\n",
    "  def get_dataset(self, seq_len=10):\n",
    "    ds_X, ds_Y = [], []\n",
    "    for i in range(2**seq_len):\n",
    "      x = [int(x) for x in list(bin(i)[2:].rjust(seq_len, '0'))]\n",
    "      ds_X.append(x)\n",
    "      ds_Y.append((np.cumsum(x)%2).tolist())\n",
    "    return np.array(ds_X), np.array(ds_Y)\n",
    "  \n",
    "enwik8 = None\n",
    "class WikipediaDataModule(BaseDataModule):\n",
    "  def get_dataset(self, seq_len=50):\n",
    "    global enwik8\n",
    "    if enwik8 is None:\n",
    "      import requests\n",
    "      enwik8_zipped = requests.get(\"https://data.deepai.org/enwik8.zip\").content\n",
    "      from zipfile import ZipFile\n",
    "      import io\n",
    "      enwik8 = ZipFile(io.BytesIO(enwik8_zipped)).read('enwik8')\n",
    "    en = np.frombuffer(enwik8, dtype=np.uint8).astype(np.int)\n",
    "    en = en[0:-seq_len+1]\n",
    "    en[en>127] = 127\n",
    "    return en[0:-1].reshape(-1, seq_len), en[1:].reshape(-1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "323554ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(queries, keys, values):\n",
    "  d = queries.shape[-1]\n",
    "  scores = torch.matmul(queries, keys.transpose(-2,-1))/math.sqrt(d)\n",
    "  attention_weights = F.softmax(scores, dim=-1)\n",
    "  return torch.matmul(attention_weights, values)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, embed_dim, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.embed_dim, self.num_heads = embed_dim, num_heads\n",
    "    assert embed_dim % num_heads == 0\n",
    "    self.projection_dim = embed_dim // num_heads\n",
    "    \n",
    "    self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "    self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "    self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "    self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "  def transpose(self, x):\n",
    "    x = x.reshape(x.shape[0], x.shape[1], self.num_heads, self.projection_dim)\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "  \n",
    "  def transpose_output(self, x):\n",
    "    x = x.permute(0, 2, 1, 3)\n",
    "    return x.reshape(x.shape[0], x.shape[1], self.embed_dim)\n",
    "    \n",
    "  def forward(self, q, k, v):\n",
    "    q = self.transpose(self.W_q(q))\n",
    "    k = self.transpose(self.W_k(k))\n",
    "    v = self.transpose(self.W_v(v))\n",
    "    output = attention(q, k, v)\n",
    "    return self.W_o(self.transpose_output(output))\n",
    "  \n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "    self.att = MultiHeadAttention(embed_dim, num_heads)\n",
    "    self.ffn = nn.Sequential(\n",
    "      nn.Linear(embed_dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, embed_dim)\n",
    "    )\n",
    "    self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "    self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "    self.dropout = nn.Dropout(rate)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.layernorm1(x + self.dropout(self.att(x, x, x)))\n",
    "    x = self.layernorm2(x + self.dropout(self.ffn(x)))\n",
    "    return x\n",
    "  \n",
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "  def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "    super(TokenAndPositionEmbedding, self).__init__()\n",
    "    self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.pos_emb = nn.Embedding(maxlen, embed_dim)\n",
    "  def forward(self, x):\n",
    "    pos = torch.arange(0, x.size(1), dtype=torch.int32)\n",
    "    return self.token_emb(x) + self.pos_emb(pos).view(1, x.size(1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "167e42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LittleTransformer(pl.LightningModule):\n",
    "  def __init__(self, seq_len=6, max_value=10, layer_count=2):\n",
    "    super().__init__()\n",
    "    self.max_value = max_value\n",
    "    self.model = nn.Sequential(\n",
    "      TokenAndPositionEmbedding(seq_len, max_value, 128),\n",
    "      *[TransformerBlock(128, 4, 32) for x in range(layer_count)],\n",
    "      nn.Linear(128, max_value),\n",
    "      nn.LogSoftmax(dim=-1))\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "  \n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    output = self.model(x)\n",
    "    loss = F.nll_loss(output.view(-1, self.max_value), y.view(-1))\n",
    "    self.log(\"train_loss\", loss)\n",
    "    return loss\n",
    "  \n",
    "  def validation_step(self, val_batch, batch_idx):\n",
    "    x, y = val_batch\n",
    "    pred = self.model(x).argmax(dim=2)\n",
    "    val_accuracy = (pred == y).type(torch.float).mean()\n",
    "    self.log(\"val_accuracy\", val_accuracy, prog_bar=True)\n",
    "  \n",
    "  def configure_optimizers(self):\n",
    "    return torch.optim.Adam(self.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2f48c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 153 K \n",
      "-------------------------------------\n",
      "153 K     Trainable params\n",
      "0         Non-trainable params\n",
      "153 K     Total params\n",
      "0.613     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c459c55b5ba4445f99c712bcc1add239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LittleTransformer(seq_len=6)\n",
    "trainer = pl.Trainer(enable_progress_bar=True, max_epochs=5)\n",
    "data = AdditionDataModule()\n",
    "#data = ReverseDataModule(cnt=1000, seq_len=20)\n",
    "#data = ParityDataModule(seq_len=14)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce48129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!rm -rf lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ddb4741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = LittleTransformer(seq_len=50, max_value=128, layer_count=6)\\ntrainer = pl.Trainer(enable_progress_bar=True, max_epochs=5)\\ndata = WikipediaDataModule()\\ntrainer.fit(model, data)\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = LittleTransformer(seq_len=50, max_value=128, layer_count=6)\n",
    "trainer = pl.Trainer(enable_progress_bar=True, max_epochs=5)\n",
    "data = WikipediaDataModule()\n",
    "trainer.fit(model, data)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6d972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
